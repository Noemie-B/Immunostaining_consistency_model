{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a230e39-cabf-46b4-82b3-8f7bdc49e02c",
   "metadata": {},
   "source": [
    "dataset : https://zenodo.org/records/10066853"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d74e511-5073-43f0-a459-d31b07a1920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import torchvision.models as models\n",
    "\n",
    "# Augmenter la limite de pixels pour éviter le warning DecompressionBombWarning\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "# Nom du dossier contenant les images HE (à ajuster si nécessaire)\n",
    "he_folder_name = \"HE\"\n",
    "\n",
    "# 1. Génération du fichier JSON des labels\n",
    "def create_label_json(dataset_dir, output_json):\n",
    "    labels = {}\n",
    "    for immunomark in os.listdir(dataset_dir):\n",
    "        immunomark_path = os.path.join(dataset_dir, immunomark)\n",
    "        if os.path.isdir(immunomark_path):\n",
    "            for img_name in os.listdir(immunomark_path):\n",
    "                if img_name.endswith(\".png\") or img_name.endswith(\".jpg\"):\n",
    "                    labels[img_name] = immunomark  # Associe l'image au type d'immunomarquage\n",
    "    \n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(labels, f, indent=4)\n",
    "    print(f\"Fichier JSON généré : {output_json}\")\n",
    "\n",
    "# Définir le chemin du dataset et du fichier JSON\n",
    "image_dir = \"C:/Users/baril/Code/SiRiC/immunomarquage/dataset_VM\"\n",
    "label_file = \"labels.json\"\n",
    "\n",
    "# Génération du fichier JSON\n",
    "create_label_json(image_dir, label_file)\n",
    "\n",
    "# 2. Téléchargement et prétraitement des données\n",
    "with open(label_file, \"r\") as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "# Création d'un dictionnaire pour convertir les noms d'immunomarquage en indices\n",
    "unique_labels = sorted(set(labels.values()) - {he_folder_name})  # Exclure HE des labels normaux\n",
    "label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "label_to_idx[he_folder_name] = -1  # Spécialisation des HE\n",
    "\n",
    "print(\"Mapping des labels:\", label_to_idx)\n",
    "\n",
    "class TMADataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, label_to_idx, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels  # Dictionnaire associant une image à son type d'immunomarquage\n",
    "        self.label_to_idx = label_to_idx  # Mapping texte -> indice\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        label_str = self.labels.get(os.path.basename(img_path), None)\n",
    "        if label_str is None or label_str not in self.label_to_idx:\n",
    "            raise ValueError(f\"Label introuvable ou incorrect pour l'image {img_path}\")\n",
    "        \n",
    "        label = self.label_to_idx[label_str]\n",
    "        \n",
    "        # Si c'est une image HE, assigner un label aléatoire pour l'entraînement\n",
    "        if label == -1:\n",
    "            label = random.choice(list(label_to_idx.values())[:-1])  # Exclure HE\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Définition des transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "valid_image_paths = []\n",
    "for img_path in image_paths:\n",
    "    try:\n",
    "        with Image.open(img_path) as img:\n",
    "            img.verify()  # Vérification de l'intégrité du fichier\n",
    "        valid_image_paths.append(img_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Image corrompue détectée et ignorée : {img_path}, Erreur: {e}\")\n",
    "\n",
    "image_paths = valid_image_paths\n",
    "#image_paths = [os.path.join(root, fname) for root, _, files in os.walk(image_dir) for fname in files if fname.endswith(\".png\")]\n",
    "\n",
    "# Utilisation d'une partie du dataset pour l'entraînement (ex: 50%)\n",
    "train_size = int(0.5 * len(image_paths))\n",
    "dataset = TMADataset(image_paths[:train_size], labels, label_to_idx, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# 3. Définition d'un Consistency Model conditionnel\n",
    "class ConsistencyModel(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConsistencyModel, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.label_embedding = torch.nn.Embedding(num_classes, 128)\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        x = self.encoder(x)\n",
    "        label_embed = self.label_embedding(labels).view(labels.size(0), 128, 1, 1)\n",
    "        x = x + label_embed  # Fusion du label avec les features\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialisation du modèle\n",
    "num_classes = len(label_to_idx) - 1  # Exclure HE du comptage\n",
    "target_classes = len(label_to_idx) - 1\n",
    "model = ConsistencyModel(target_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Ajout d'une perte perceptuelle équilibrée\n",
    "vgg = models.vgg16(weights=models.VGG16_Weights.DEFAULT).features[:16].to(device)\n",
    "vgg.eval()\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def perceptual_loss(output, target):\n",
    "    return 0.1 * torch.nn.functional.mse_loss(vgg(output), vgg(target))\n",
    "\n",
    "# 4. Entraînement du modèle et sauvegarde des checkpoints\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "criterion = torch.nn.L1Loss()\n",
    "checkpoint_dir = \"checkpoints_TMA\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "def train_model(model, dataloader, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for imgs, labels in dataloader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            output = model(imgs, labels)\n",
    "            loss = criterion(output, imgs) + perceptual_loss(output, imgs)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "        torch.save(model.state_dict(), os.path.join(checkpoint_dir, f\"model_epoch_{epoch+1}.pth\"))\n",
    "\n",
    "train_model(model, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6cd35c-24a2-478c-8c4f-aa1e2ff4362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Test du modèle sur une image inédite\n",
    "def test_model(model, image_path, target_label):\n",
    "    model.eval()\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "    label_idx = torch.tensor([label_to_idx[target_label]], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(img, label_idx)\n",
    "    output_img = transforms.ToPILImage()(output.squeeze(0).cpu())\n",
    "    output_img.save(\"output_image_p53.png\")\n",
    "    print(f\"Image enregistrée sous output_image.png avec immunomarquage : {target_label}\")\n",
    "\n",
    "# Exemple d'utilisation\n",
    "test_image = \"C:/Users/baril/Code/SiRiC/immunomarquage/dataset_VM/HE/B3_TMA_15_02_IB_HE.png\"  # Remplace par un chemin valide\n",
    "test_label = \"p53\"  # Remplace par un immunomarquage existant\n",
    "test_model(model, test_image, test_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf578f1-7f1a-4f6d-821c-a4edbf5a45e3",
   "metadata": {},
   "source": [
    "Avec seuillage d'Otsu pour garantir que le fond blanc reste blanc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b1c90cc-db4f-43c2-b633-6a22b155550a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier JSON généré : labels.json\n",
      "Mapping des labels: {'AR': 0, 'CD146': 1, 'CD44': 2, 'ERG': 3, 'NKX3': 4, 'p53': 5, 'HE': 6}\n",
      "Image corrompue détectée et ignorée : C:/Users/baril/Code/SiRiC/immunomarquage/dataset_VM\\CD44\\cd4.png, Erreur: cannot identify image file 'C:\\\\Users\\\\baril\\\\Code\\\\SiRiC\\\\immunomarquage\\\\dataset_VM\\\\CD44\\\\cd4.png'\n",
      "Epoch 1, Loss: 0.07040721983096357\n",
      "Epoch 2, Loss: 0.03457504056267819\n",
      "Epoch 3, Loss: 0.027783501056670133\n",
      "Epoch 4, Loss: 0.022905063908547164\n",
      "Epoch 5, Loss: 0.020548305265858012\n",
      "Epoch 6, Loss: 0.018575589708447204\n",
      "Epoch 7, Loss: 0.01676779358284706\n",
      "Epoch 8, Loss: 0.016219634880801127\n",
      "Epoch 9, Loss: 0.0150031102221396\n",
      "Epoch 10, Loss: 0.01447367490569161\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Augmenter la limite de pixels pour éviter le warning DecompressionBombWarning\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "# Nom du dossier contenant les images HE (à ajuster si nécessaire)\n",
    "he_folder_name = \"HE\"\n",
    "\n",
    "# 1. Génération du fichier JSON des labels\n",
    "def create_label_json(dataset_dir, output_json):\n",
    "    labels = {}\n",
    "    for immunomark in os.listdir(dataset_dir):\n",
    "        immunomark_path = os.path.join(dataset_dir, immunomark)\n",
    "        if os.path.isdir(immunomark_path):\n",
    "            for img_name in os.listdir(immunomark_path):\n",
    "                if img_name.endswith(\".png\") or img_name.endswith(\".jpg\"):\n",
    "                    labels[img_name] = immunomark  # Associe l'image au type d'immunomarquage\n",
    "    \n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(labels, f, indent=4)\n",
    "    print(f\"Fichier JSON généré : {output_json}\")\n",
    "\n",
    "# Définir le chemin du dataset et du fichier JSON\n",
    "image_dir = \"C:/Users/baril/Code/SiRiC/immunomarquage/dataset_VM\"\n",
    "label_file = \"labels.json\"\n",
    "\n",
    "# Génération du fichier JSON\n",
    "create_label_json(image_dir, label_file)\n",
    "\n",
    "# 2. Téléchargement et prétraitement des données\n",
    "with open(label_file, \"r\") as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "# Création d'un dictionnaire pour convertir les noms d'immunomarquage en indices\n",
    "unique_labels = sorted(set(labels.values()) - {he_folder_name})  # Exclure HE des labels normaux\n",
    "label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "label_to_idx[he_folder_name] = len(unique_labels)  # Assigner un index spécial à HE\n",
    "\n",
    "print(\"Mapping des labels:\", label_to_idx)\n",
    "\n",
    "class TMADataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, label_to_idx, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels  # Dictionnaire associant une image à son type d'immunomarquage\n",
    "        self.label_to_idx = label_to_idx  # Mapping texte -> indice\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        label_str = self.labels.get(os.path.basename(img_path), None)\n",
    "        if label_str is None or label_str not in self.label_to_idx:\n",
    "            raise ValueError(f\"Label introuvable ou incorrect pour l'image {img_path}\")\n",
    "        \n",
    "        label = self.label_to_idx[label_str]\n",
    "        \n",
    "        # Si c'est une image HE, assigner un label aléatoire pour l'entraînement\n",
    "        if label == len(unique_labels):\n",
    "            label = random.choice(list(label_to_idx.values())[:-1])  # Exclure HE\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Définition des transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def detect_white_background(image):\n",
    "    \"\"\"Détecte les pixels de fond blanc dans une image et retourne un masque booléen.\"\"\"\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        raise ValueError(\"L'image d'entrée doit avoir 3 canaux (RGB)\")\n",
    "    \n",
    "    _, mask = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY)\n",
    "    return mask.astype(bool)\n",
    "\n",
    "image_paths = [os.path.join(root, fname) for root, _, files in os.walk(image_dir) for fname in files if fname.endswith((\".png\", \".jpg\"))]\n",
    "\n",
    "valid_image_paths = []\n",
    "for img_path in image_paths:\n",
    "    try:\n",
    "        with Image.open(img_path) as img:\n",
    "            img.verify()  # Vérification de l'intégrité du fichier\n",
    "        valid_image_paths.append(img_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Image corrompue détectée et ignorée : {img_path}, Erreur: {e}\")\n",
    "\n",
    "image_paths = valid_image_paths\n",
    "\n",
    "# Création du DataLoader\n",
    "dataset = TMADataset(image_paths, labels, label_to_idx, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# 3. Définition d'un Consistency Model conditionnel\n",
    "class ConsistencyModel(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConsistencyModel, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.label_embedding = torch.nn.Embedding(num_classes, 128)\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        x = self.encoder(x)\n",
    "        is_he = (labels == len(unique_labels))\n",
    "        if not torch.any(is_he):\n",
    "            label_embed = self.label_embedding(labels).view(labels.size(0), 128, 1, 1)\n",
    "            x = x + label_embed  # Fusion du label avec les features\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialisation du modèle\n",
    "num_classes = len(label_to_idx) - 1  # Exclure HE du comptage\n",
    "model = ConsistencyModel(num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Définition de la fonction de perte et de l'optimiseur\n",
    "criterion = torch.nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Fonction d'entraînement\n",
    "def train_model(model, dataloader, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for imgs, labels in dataloader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            output = model(imgs, labels)\n",
    "            loss = criterion(output, imgs)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(dataloader)}\")\n",
    "\n",
    "# Lancement de l'entraînement\n",
    "train_model(model, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "815e48f6-bb69-4f50-8ad9-5d036ea4f2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baril\\AppData\\Local\\Temp\\ipykernel_11360\\3987411327.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Charger le modèle\n",
    "checkpoint_path = \"checkpoints_TMA/model_epoch_10.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Charger le state_dict pour détecter le nombre de classes\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "num_classes = checkpoint['label_embedding.weight'].shape[0]  # Déduire le nombre de classes\n",
    "\n",
    "class ConsistencyModel(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConsistencyModel, self).__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.label_embedding = torch.nn.Embedding(num_classes, 128)\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        x = self.encoder(x)\n",
    "        label_embed = self.label_embedding(labels).view(labels.size(0), 128, 1, 1)\n",
    "        x = x + label_embed  # Fusion du label avec les features\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Charger le modèle sauvegardé\n",
    "model = ConsistencyModel(num_classes)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Transformer une image HE\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def detect_white_background(image):\n",
    "    \"\"\"Détecte les pixels de fond blanc dans une image et retourne un masque booléen.\"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    _, mask = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY)\n",
    "    return mask.astype(bool)\n",
    "\n",
    "def apply_immunostaining(image_path, chosen_label):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_resized = img.resize((256, 256))\n",
    "    img_np = np.array(img_resized)\n",
    "    mask = detect_white_background(img_np)\n",
    "    img_tensor = transform(img_resized).unsqueeze(0).to(device)\n",
    "    \n",
    "    label_to_idx = {\"AR\": 0, \"CD146\": 1, \"CD44\": 2, \"ERG\": 3, \"NKX3\": 4, \"p53\": 5}  # HE exclu\n",
    "    label_idx = torch.tensor([label_to_idx[chosen_label]], dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor, label_idx)\n",
    "    \n",
    "    output_img = output.squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
    "    output_img = (output_img * 255).astype(np.uint8)\n",
    "    \n",
    "    # Appliquer le masque : garder le fond blanc\n",
    "    output_img[mask] = [255, 255, 255]\n",
    "    \n",
    "    output_pil = Image.fromarray(output_img.astype(np.uint8))\n",
    "    return output_pil\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7851dc2-9800-4ddc-8d45-bcb16ab2c670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1_TMA_15_02_IIIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "E6_TMA_15_02_IB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "E9_TMA_15_02_IIIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "F2_TMA_15_02_IIIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "F6_TMA_15_02_IIIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "G11_TMA_15_02_IIIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "G12_TMA_15_02_IIIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "G1_TMA_15_02_IIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "G5_TMA_15_02_IIIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "H10_TMA_15_02_IB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "H4_TMA_15_02_IB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "I3_TMA_15_02_IIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "I9_TMA_15_02_IIIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "J10_TMA_15_02_IIIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "J3_TMA_15_02_IIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "K11_TMA_15_02_IIIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "K12_TMA_15_02_IB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "K6_TMA_15_02_IB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "L7_TMA_15_02_IIIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "M4_TMA_15_02_IB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "M5_TMA_15_02_IB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "N1_TMA_15_02_IIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "N3_TMA_15_02_IIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "N6_TMA_15_02_IB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "N6_TMA_15_02_IIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "O2_TMA_15_02_IIIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "O8_TMA_15_02_IIIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "P12_TMA_15_02_IIIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "P4_TMA_15_02_IB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "Q8_TMA_15_02_IB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "R4_TMA_15_02_IIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "R9_TMA_15_02_IIIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "S12_TMA_15_02_IIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "T12_TMA_15_02_IIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n",
      "T8_TMA_15_02_IIB_HE.png\n",
      "AR\n",
      "CD146\n",
      "CD44\n",
      "ERG\n",
      "NKX3\n",
      "p53\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'utilisation\n",
    "imgs = os.listdir('C:/Users/baril/Code/SiRiC/immunomarquage/HE_test_VM/')\n",
    "for img in imgs[10:] :\n",
    "    print(img)\n",
    "    image_path = \"C:/Users/baril/Code/SiRiC/immunomarquage/HE_test_VM/\"+img  # Remplace par le chemin de ton image HE\n",
    "\n",
    "    labels = ['AR','CD146','CD44','ERG','NKX3','p53']\n",
    "    for label in labels :\n",
    "        chosen_label = label  # Choisir l'immunomarquage voulu\n",
    "        print(chosen_label)\n",
    "        output_image = apply_immunostaining(image_path, chosen_label)\n",
    "        #output_image.show()  # Afficher l'image générée\n",
    "        output_dir = 'output_immunostained/'+chosen_label+'/'\n",
    "        if not os.path.exists(output_dir) :\n",
    "            os.mkdir(output_dir)\n",
    "        output_image.save(output_dir+img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0887b7-2872-4be2-b126-6d0d516df204",
   "metadata": {},
   "outputs": [],
   "source": [
    "'AR': 0, 'CD146': 1, 'CD44': 2, 'ERG': 3, 'NKX3': 4, 'p53': 5, 'HE': 6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
